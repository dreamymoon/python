                           ———————————————— week 1 ————————————————
JSON stands for JavaScript Object Notation，在这个module里有两个主要functions：dumps和loads
写代码时要先import json
json.loads() takes a string as input and produces a python object (a dictionary or a list) as output. json.dumps()相反
————
import json
def pretty(obj):
    return json.dumps(obj, sort_keys=True, indent=2)  #sort_keys=True是按照字母表排列，先数字后字母，indent表示后面有多少空格，这样方便阅读

d = {'key1': {'c': True, 'a': 90, '5': 50}, 'key2':{'b': 3, 'c': "yes"}}

print(d)
print('--------')
print(pretty(d))
————shallow-copy 与 deep-copy
import copy #引入copy模块
original = [['canines', ['dogs', 'puppies']], ['felines', ['cats', 'kittens']]]
shallow_copy_version = original[:]  # shallow-copy is original-> False;shallow-copy == original -> True
deeply_copied_version = copy.deepcopy(original)  #copy模块下的deepcopy功能
original.append("Hi there")
original[0].append(["marsupials"])
print("-------- Original -----------")
print(original)
print("-------- deep copy -----------")
print(deeply_copied_version)
print("-------- shallow copy -----------")
print(shallow_copy_version)
————以下为显示结果，可以看得deep-copy是不受original改变的影响的；
————而shallow-copy受到original部分影响：往original这个list加元素，shallow不变；往original的子list加元素，shallow跟着变化
-------- Original -----------
[['canines', ['dogs', 'puppies'], ['marsupials']], ['felines', ['cats', 'kittens']], 'Hi there']
-------- deep copy -----------
[['canines', ['dogs', 'puppies']], ['felines', ['cats', 'kittens']]]
-------- shallow copy -----------
[['canines', ['dogs', 'puppies'], ['marsupials']], ['felines', ['cats', 'kittens']]]
————
extracting from nested data要看每一层是什么类型，如果是dictionary，print .keys；如果是list，print（len（list））
第一周的课后练习Nested Data and Iteration中第四题挺难的，需要思考

                              ———————————————— week 2 ————————————————
————map function 
def triple(value):
    return 3*value

def tripleStuff(a_list):
    new_seq = map(triple, a_list)  #map(transformer,sequence)
    return list(new_seq)

things = [2, 5, 9]
things3 = tripleStuff(things)
print(things3)
————filter function
def keep_evens(nums):
    new_seq = filter(lambda num: num % 2 == 0, nums)  #filter (some filtration function ,squence)
    return list(new_seq)

print(keep_evens([3, 4, 6, 7, 0, 1]))
————List Comprehensions，集map和filter功能于一身
常用格式 [<transformer_expression> for <varname> in <sequence> if <filteration_expression>]
————zip function
L1 = [3, 4, 5]
L2 = [1, 2, 3]
L4 = list(zip(L1, L2))
print(L4)               #结果为[(3, 1), (4, 2), (5, 3)]
————L1和l2相加，如何用zip以及list comprehensions
L1 = [3, 4, 5]
L2 = [1, 2, 3]
L3 = [x1 + x2 for (x1, x2) in list(zip(L1, L2))]
print(L3)
                               ———————————————— week 3 ————————————————
了解REST APIs: URLs, Domain names, IP address
————fetching a page
import requests # 这个模块有这些功能.text .url .json() .status_code .headers .history 
import json

page = requests.get("https://api.datamuse.com/words?rel_rhy=funny")  #用request.get时， the first argument是问号后面的
print(type(page))       #这里的类型是'requests.Response'
print(page.text[:150]) # print the first 150 characters
print(page.url) # print the url that was fetched
print("------")
x = page.json() # turn page.text into a python object，这里是json.loads(page.txt)的简要写法
print(type(x))  # <class 'list'>
print("---first item in the list---")
print(x[0])
print("---the whole list, pretty printed---")
print(json.dumps(x, indent=2)) # pretty print the results
————
# import statements for necessary Python modules
import requests

def get_rhymes(word):
    baseurl = "https://api.datamuse.com/words"
    params_diction = {} # Set up an empty dictionary for query parameters
    params_diction["rel_rhy"] = word
    params_diction["max"] = "3" # get at most 3 results
    resp = requests.get(baseurl, params=params_diction) #注意这里的params要是dictionary，并且里面都是string，否则不会返回结果
    # return the top three words
    word_ds = resp.json()
    return [d['word'] for d in word_ds]
    return resp.json() # Return a python object (a list of dictionaries in this case)

print(get_rhymes("funny"))
————
有个模块叫做request_with_caching 
there are helper functions _write_to_file and read_to_file that write a cache dictionary to and read it from a file.
Instead of invoking requests.get(), you’ll invoke requests_with_caching.get()
运行后会返回3种结果：
1.found in permanent cache 2.found in page-specific cache 3.new; adding to cache
requests_with_caching.get()还有其他功能：cache_file  private_keys_to_ignore
————
# import statements
import requests_with_caching
import json
# import webbrowser 这个功能教材上暂不支持

# apply for a flickr authentication key at http://www.flickr.com/services/apps/create/apply/?
# paste the key (not the secret) as the value of the variable flickr_key
flickr_key = 'yourkeyhere'

def get_flickr_data(tags_string):
    baseurl = "https://api.flickr.com/services/rest/"
    params_diction = {}
    params_diction["api_key"] = flickr_key # from the above global variable
    params_diction["tags"] = tags_string # must be a comma separated string to work correctly
    params_diction["tag_mode"] = "all"
    params_diction["method"] = "flickr.photos.search"
    params_diction["per_page"] = 5
    params_diction["media"] = "photos"
    params_diction["format"] = "json"
    params_diction["nojsoncallback"] = 1
    flickr_resp = requests_with_caching.get(baseurl, params = params_diction, permanent_cache_file="flickr_cache.txt")
    # Useful for debugging: print the url! Uncomment the below line to do so.
    print(flickr_resp.url) # Paste the result into the browser to check it out...
    return flickr_resp.json()

result_river_mts = get_flickr_data("river,mountains")

# Some code to open up a few photos that are tagged with the mountains and river tags...

photos = result_river_mts['photos']['photo']
for photo in photos:
    owner = photo['owner']
    photo_id = photo['id']
    url = 'https://www.flickr.com/photos/{}/{}'.format(owner, photo_id)
    print(url)
    # webbrowser.open(url)  这个功能教材上暂不支持
————
第三周的最后作业也很费时，对提高写代码能力很有帮助！
